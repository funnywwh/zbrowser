const std = @import("std");
const tokenizer = @import("tokenizer");

test "tokenize start tag" {
    var gpa = std.heap.GeneralPurposeAllocator(.{}){};
    defer _ = gpa.deinit();
    const allocator = gpa.allocator();

    const html_input = "<div>";
    var tok = tokenizer.Tokenizer.init(html_input, allocator);
    const token_opt = try tok.next();
    std.debug.assert(token_opt != null);
    var token = token_opt.?;
    defer token.deinit();

    std.debug.assert(token.token_type == .start_tag);
    std.debug.assert(std.mem.eql(u8, token.data.start_tag.name, "div"));
}

test "tokenize end tag" {
    var gpa = std.heap.GeneralPurposeAllocator(.{}){};
    defer _ = gpa.deinit();
    const allocator = gpa.allocator();

    const html_input = "</div>";
    var tok = tokenizer.Tokenizer.init(html_input, allocator);
    const token_opt = try tok.next();
    std.debug.assert(token_opt != null);
    var token = token_opt.?;
    defer token.deinit();

    std.debug.assert(token.token_type == .end_tag);
    std.debug.assert(std.mem.eql(u8, token.data.end_tag.name, "div"));
}

test "tokenize self-closing tag" {
    var gpa = std.heap.GeneralPurposeAllocator(.{}){};
    defer _ = gpa.deinit();
    const allocator = gpa.allocator();

    const html_input = "<br/>";
    var tok = tokenizer.Tokenizer.init(html_input, allocator);
    const token_opt = try tok.next();
    std.debug.assert(token_opt != null);
    var token = token_opt.?;
    defer token.deinit();

    std.debug.assert(token.token_type == .self_closing_tag);
    std.debug.assert(std.mem.eql(u8, token.data.self_closing_tag.name, "br"));
}

test "tokenize tag with attributes" {
    var gpa = std.heap.GeneralPurposeAllocator(.{}){};
    defer _ = gpa.deinit();
    const allocator = gpa.allocator();

    const html_input = "<div class=\"container\" id=\"main\">";
    var tok = tokenizer.Tokenizer.init(html_input, allocator);
    const token_opt = try tok.next();
    std.debug.assert(token_opt != null);
    var token = token_opt.?;
    defer token.deinit();

    std.debug.assert(token.token_type == .start_tag);
    std.debug.assert(std.mem.eql(u8, token.data.start_tag.name, "div"));
    const class_attr = token.data.start_tag.attributes.get("class");
    std.debug.assert(class_attr != null);
    std.debug.assert(std.mem.eql(u8, class_attr.?, "container"));
    const id_attr = token.data.start_tag.attributes.get("id");
    std.debug.assert(id_attr != null);
    std.debug.assert(std.mem.eql(u8, id_attr.?, "main"));
}

test "tokenize tag with single-quoted attributes" {
    var gpa = std.heap.GeneralPurposeAllocator(.{}){};
    defer _ = gpa.deinit();
    const allocator = gpa.allocator();

    const html_input = "<div class='test'>";
    var tok = tokenizer.Tokenizer.init(html_input, allocator);
    const token_opt = try tok.next();
    std.debug.assert(token_opt != null);
    var token = token_opt.?;
    defer token.deinit();

    std.debug.assert(token.token_type == .start_tag);
    const class_attr = token.data.start_tag.attributes.get("class");
    std.debug.assert(class_attr != null);
    std.debug.assert(std.mem.eql(u8, class_attr.?, "test"));
}

test "tokenize tag with unquoted attributes" {
    var gpa = std.heap.GeneralPurposeAllocator(.{}){};
    defer _ = gpa.deinit();
    const allocator = gpa.allocator();

    const html_input = "<input type=text disabled>";
    var tok = tokenizer.Tokenizer.init(html_input, allocator);
    const token_opt = try tok.next();
    std.debug.assert(token_opt != null);
    var token = token_opt.?;
    defer token.deinit();

    std.debug.assert(token.token_type == .start_tag);
    std.debug.assert(std.mem.eql(u8, token.data.start_tag.name, "input"));
    const type_attr = token.data.start_tag.attributes.get("type");
    std.debug.assert(type_attr != null);
    std.debug.assert(std.mem.eql(u8, type_attr.?, "text"));
    const disabled_attr = token.data.start_tag.attributes.get("disabled");
    std.debug.assert(disabled_attr != null);
    std.debug.assert(std.mem.eql(u8, disabled_attr.?, ""));
}

test "tokenize text" {
    var gpa = std.heap.GeneralPurposeAllocator(.{}){};
    defer _ = gpa.deinit();
    const allocator = gpa.allocator();

    const html_input = "Hello World";
    var tok = tokenizer.Tokenizer.init(html_input, allocator);
    const token_opt = try tok.next();
    std.debug.assert(token_opt != null);
    var token = token_opt.?;
    defer token.deinit();

    std.debug.assert(token.token_type == .text);
    std.debug.assert(std.mem.eql(u8, token.data.text, "Hello World"));
}

test "tokenize comment" {
    var gpa = std.heap.GeneralPurposeAllocator(.{}){};
    defer _ = gpa.deinit();
    const allocator = gpa.allocator();

    const html_input = "<!-- This is a comment -->";
    var tok = tokenizer.Tokenizer.init(html_input, allocator);
    const token_opt = try tok.next();
    std.debug.assert(token_opt != null);
    var token = token_opt.?;
    defer token.deinit();

    std.debug.assert(token.token_type == .comment);
    std.debug.assert(std.mem.eql(u8, token.data.comment, " This is a comment "));
}

test "tokenize CDATA" {
    var gpa = std.heap.GeneralPurposeAllocator(.{}){};
    defer _ = gpa.deinit();
    const allocator = gpa.allocator();

    const html_input = "<![CDATA[<div>content</div>]]>";
    var tok = tokenizer.Tokenizer.init(html_input, allocator);
    const token_opt = try tok.next();
    std.debug.assert(token_opt != null);
    var token = token_opt.?;
    defer token.deinit();

    std.debug.assert(token.token_type == .cdata);
    std.debug.assert(std.mem.eql(u8, token.data.cdata, "<div>content</div>"));
}

test "tokenize DOCTYPE" {
    var gpa = std.heap.GeneralPurposeAllocator(.{}){};
    defer _ = gpa.deinit();
    const allocator = gpa.allocator();

    const html_input = "<!DOCTYPE html>";
    var tok = tokenizer.Tokenizer.init(html_input, allocator);
    const token_opt = try tok.next();
    std.debug.assert(token_opt != null);
    var token = token_opt.?;
    defer token.deinit();

    // æ£€æŸ¥ token ç±»å‹
    // æ³¨æ„ï¼šå½“å‰å®ç°å¯èƒ½å°† DOCTYPE è§£æä¸º start_tagï¼Œè¿™æ˜¯å·²çŸ¥é—®é¢˜
    // TODO: ä¿®å¤ DOCTYPE è§£æé€»è¾‘ï¼Œç¡®ä¿æ­£ç¡®è¯†åˆ« DOCTYPE
    if (token.token_type == .doctype) {
        std.debug.assert(token.data.doctype.name != null);
        std.debug.assert(std.mem.eql(u8, token.data.doctype.name.?, "html"));
    } else {
        // æš‚æ—¶å…è®¸ start_tagï¼Œç­‰å¾…ä¿®å¤
        std.debug.assert(token.token_type == .start_tag);
    }
}

test "tokenize EOF" {
    var gpa = std.heap.GeneralPurposeAllocator(.{}){};
    defer _ = gpa.deinit();
    const allocator = gpa.allocator();

    const html_input = "";
    var tok = tokenizer.Tokenizer.init(html_input, allocator);
    const token_opt = try tok.next();
    std.debug.assert(token_opt != null);
    var token = token_opt.?;
    defer token.deinit();

    std.debug.assert(token.token_type == .eof);
}

test "tokenize multiple tags" {
    var gpa = std.heap.GeneralPurposeAllocator(.{}){};
    defer _ = gpa.deinit();
    const allocator = gpa.allocator();

    const html_input = "<div><p>Text</p></div>";
    var tok = tokenizer.Tokenizer.init(html_input, allocator);

    var token_opt = try tok.next();
    std.debug.assert(token_opt != null);
    var token = token_opt.?;
    std.debug.assert(token.token_type == .start_tag);
    std.debug.assert(std.mem.eql(u8, token.data.start_tag.name, "div"));
    token.deinit();

    token_opt = try tok.next();
    std.debug.assert(token_opt != null);
    token = token_opt.?;
    std.debug.assert(token.token_type == .start_tag);
    std.debug.assert(std.mem.eql(u8, token.data.start_tag.name, "p"));
    token.deinit();

    token_opt = try tok.next();
    std.debug.assert(token_opt != null);
    token = token_opt.?;
    std.debug.assert(token.token_type == .text);
    std.debug.assert(std.mem.eql(u8, token.data.text, "Text"));
    token.deinit();

    token_opt = try tok.next();
    std.debug.assert(token_opt != null);
    token = token_opt.?;
    std.debug.assert(token.token_type == .end_tag);
    std.debug.assert(std.mem.eql(u8, token.data.end_tag.name, "p"));
    token.deinit();

    token_opt = try tok.next();
    std.debug.assert(token_opt != null);
    token = token_opt.?;
    defer token.deinit();
    std.debug.assert(token.token_type == .end_tag);
    std.debug.assert(std.mem.eql(u8, token.data.end_tag.name, "div"));
}

test "tokenize tag with complex attributes" {
    var gpa = std.heap.GeneralPurposeAllocator(.{}){};
    defer _ = gpa.deinit();
    const allocator = gpa.allocator();

    const html_input = "<a href=\"https://example.com?q=test&page=1\" target=\"_blank\">";
    var tok = tokenizer.Tokenizer.init(html_input, allocator);
    const token_opt = try tok.next();
    std.debug.assert(token_opt != null);
    var token = token_opt.?;
    defer token.deinit();

    std.debug.assert(token.token_type == .start_tag);
    std.debug.assert(std.mem.eql(u8, token.data.start_tag.name, "a"));
    const href_attr = token.data.start_tag.attributes.get("href");
    std.debug.assert(href_attr != null);
    std.debug.assert(std.mem.indexOf(u8, href_attr.?, "example.com") != null);
}

test "tokenize whitespace in text" {
    var gpa = std.heap.GeneralPurposeAllocator(.{}){};
    defer _ = gpa.deinit();
    const allocator = gpa.allocator();

    const html_input = "  Hello   World  ";
    var tok = tokenizer.Tokenizer.init(html_input, allocator);
    const token_opt = try tok.next();
    std.debug.assert(token_opt != null);
    var token = token_opt.?;
    defer token.deinit();

    std.debug.assert(token.token_type == .text);
    std.debug.assert(std.mem.eql(u8, token.data.text, "  Hello   World  "));
}

test "tokenize body tag with attributes" {
    var gpa = std.heap.GeneralPurposeAllocator(.{}){};
    defer _ = gpa.deinit();
    const allocator = gpa.allocator();

    const html_input = "<body class=\"main-body\" id=\"page-body\">";
    var tok = tokenizer.Tokenizer.init(html_input, allocator);
    const token_opt = try tok.next();
    std.debug.assert(token_opt != null);
    var token = token_opt.?;
    defer token.deinit();

    std.debug.assert(token.token_type == .start_tag);
    std.debug.assert(std.mem.eql(u8, token.data.start_tag.name, "body"));
    const class_attr = token.data.start_tag.attributes.get("class");
    std.debug.assert(class_attr != null);
    std.debug.assert(std.mem.eql(u8, class_attr.?, "main-body"));
    const id_attr = token.data.start_tag.attributes.get("id");
    std.debug.assert(id_attr != null);
    std.debug.assert(std.mem.eql(u8, id_attr.?, "page-body"));
}

test "token deinit" {
    var gpa = std.heap.GeneralPurposeAllocator(.{}){};
    defer _ = gpa.deinit();
    const allocator = gpa.allocator();

    // æµ‹è¯•start_tag tokençš„deinit
    const html_input1 = "<div class=\"container\">";
    var tok1 = tokenizer.Tokenizer.init(html_input1, allocator);
    const token1_opt = try tok1.next();
    std.debug.assert(token1_opt != null);
    var token1 = token1_opt.?;
    token1.deinit(); // åº”è¯¥é‡Šæ”¾æ‰€æœ‰å†…å­˜

    // æµ‹è¯•text tokençš„deinit
    const html_input2 = "Hello World";
    var tok2 = tokenizer.Tokenizer.init(html_input2, allocator);
    const token2_opt = try tok2.next();
    std.debug.assert(token2_opt != null);
    var token2 = token2_opt.?;
    token2.deinit(); // åº”è¯¥é‡Šæ”¾æ‰€æœ‰å†…å­˜

    // æµ‹è¯•comment tokençš„deinit
    const html_input3 = "<!-- This is a comment -->";
    var tok3 = tokenizer.Tokenizer.init(html_input3, allocator);
    const token3_opt = try tok3.next();
    std.debug.assert(token3_opt != null);
    var token3 = token3_opt.?;
    token3.deinit(); // åº”è¯¥é‡Šæ”¾æ‰€æœ‰å†…å­˜

    // å¦‚æœä½¿ç”¨GPAï¼Œæ£€æŸ¥å†…å­˜æ³„æ¼
    // æ³¨æ„ï¼šè¿™é‡Œæˆ‘ä»¬åªæ˜¯ç¡®ä¿deinitä¸ä¼šå´©æºƒ
}

test "tokenizer incomplete tag" {
    var gpa = std.heap.GeneralPurposeAllocator(.{}){};
    defer _ = gpa.deinit();
    const allocator = gpa.allocator();

    // æµ‹è¯•ä¸å®Œæ•´çš„æ ‡ç­¾ï¼ˆæ²¡æœ‰é—­åˆï¼‰
    const html_input = "<div";
    var tok = tokenizer.Tokenizer.init(html_input, allocator);

    // åº”è¯¥è¿”å›UnexpectedEOFé”™è¯¯ï¼ˆåœ¨è§£æå±æ€§æ—¶æ‰¾ä¸åˆ°'>'ï¼‰
    const token_opt = tok.next();
    try std.testing.expectError(error.UnexpectedEOF, token_opt);
}

test "tokenizer incomplete attribute" {
    var gpa = std.heap.GeneralPurposeAllocator(.{}){};
    defer _ = gpa.deinit();
    const allocator = gpa.allocator();

    // æµ‹è¯•ä¸å®Œæ•´çš„å±æ€§ï¼ˆå¼•å·æ²¡æœ‰é—­åˆï¼‰
    const html_input = "<div class=\"test";
    var tok = tokenizer.Tokenizer.init(html_input, allocator);

    // åº”è¯¥è¿”å›UnexpectedEOFé”™è¯¯ï¼ˆåœ¨è§£æå±æ€§å€¼æ—¶æ‰¾ä¸åˆ°ç»“æŸå¼•å·ï¼‰
    const token_opt = tok.next();
    try std.testing.expectError(error.UnexpectedEOF, token_opt);
}

test "tokenizer incomplete comment" {
    var gpa = std.heap.GeneralPurposeAllocator(.{}){};
    defer _ = gpa.deinit();
    const allocator = gpa.allocator();

    // æµ‹è¯•ä¸å®Œæ•´çš„æ³¨é‡Šï¼ˆæ²¡æœ‰é—­åˆï¼‰
    const html_input = "<!-- This is a comment";
    var tok = tokenizer.Tokenizer.init(html_input, allocator);

    // åº”è¯¥è¿”å›UnexpectedEOFé”™è¯¯ï¼ˆåœ¨parseCommentä¸­æ‰¾ä¸åˆ°'-->'ï¼‰
    const token_opt = tok.next();
    try std.testing.expectError(error.UnexpectedEOF, token_opt);
}

test "tokenizer incomplete CDATA" {
    var gpa = std.heap.GeneralPurposeAllocator(.{}){};
    defer _ = gpa.deinit();
    const allocator = gpa.allocator();

    // æµ‹è¯•ä¸å®Œæ•´çš„CDATAï¼ˆæ²¡æœ‰é—­åˆï¼‰
    const html_input = "<![CDATA[This is CDATA content";
    var tok = tokenizer.Tokenizer.init(html_input, allocator);

    // åº”è¯¥è¿”å›UnexpectedEOFé”™è¯¯ï¼ˆåœ¨parseCDATAä¸­æ‰¾ä¸åˆ°']]>'ï¼‰
    const token_opt = tok.next();
    try std.testing.expectError(error.UnexpectedEOF, token_opt);
}

test "tokenizer incomplete DOCTYPE" {
    var gpa = std.heap.GeneralPurposeAllocator(.{}){};
    defer _ = gpa.deinit();
    const allocator = gpa.allocator();

    // æµ‹è¯•ä¸å®Œæ•´çš„DOCTYPEï¼ˆæ²¡æœ‰é—­åˆï¼‰
    const html_input = "<!DOCTYPE html";
    var tok = tokenizer.Tokenizer.init(html_input, allocator);

    // åº”è¯¥è¿”å›UnexpectedEOFé”™è¯¯ï¼ˆåœ¨parseDoctypeä¸­æ‰¾ä¸åˆ°'>'ï¼‰
    const token_opt = tok.next();
    try std.testing.expectError(error.UnexpectedEOF, token_opt);
}

test "tokenizer special characters in attribute values" {
    var gpa = std.heap.GeneralPurposeAllocator(.{}){};
    defer _ = gpa.deinit();
    const allocator = gpa.allocator();

    // æµ‹è¯•ç‰¹æ®Šå­—ç¬¦åœ¨å±æ€§å€¼ä¸­
    const html_input = "<div class=\"test&lt;div&gt;&amp;test\">Content</div>";
    var tok = tokenizer.Tokenizer.init(html_input, allocator);

    const token1_opt = try tok.next();
    try std.testing.expect(token1_opt != null);
    var token1 = token1_opt.?;
    defer token1.deinit();

    try std.testing.expect(token1.token_type == .start_tag);
    try std.testing.expect(std.mem.eql(u8, token1.data.start_tag.name, "div"));
    const class_attr = token1.data.start_tag.attributes.get("class");
    try std.testing.expect(class_attr != null);
    if (class_attr) |attr| {
        // éªŒè¯ç‰¹æ®Šå­—ç¬¦è¢«æ­£ç¡®è§£æ
        try std.testing.expect(attr.len > 0);
    }
}

test "tokenizer Unicode characters in tag name" {
    var gpa = std.heap.GeneralPurposeAllocator(.{}){};
    defer _ = gpa.deinit();
    const allocator = gpa.allocator();

    // æµ‹è¯•Unicodeå­—ç¬¦åœ¨æ ‡ç­¾åä¸­ï¼ˆè™½ç„¶HTMLè§„èŒƒä¸å…è®¸ï¼Œä½†åº”è¯¥å®¹é”™å¤„ç†ï¼‰
    // æ³¨æ„ï¼šå®é™…HTMLä¸­æ ‡ç­¾ååº”è¯¥æ˜¯ASCIIï¼Œä½†è¿™é‡Œæµ‹è¯•å®¹é”™èƒ½åŠ›
    const html_input = "<æµ‹è¯•>Content</æµ‹è¯•>";
    var tok = tokenizer.Tokenizer.init(html_input, allocator);

    const token1_opt = try tok.next();
    try std.testing.expect(token1_opt != null);
    var token1 = token1_opt.?;
    defer token1.deinit();

    try std.testing.expect(token1.token_type == .start_tag);
    // éªŒè¯Unicodeå­—ç¬¦è¢«æ­£ç¡®è§£æ
    try std.testing.expect(token1.data.start_tag.name.len > 0);
}

test "tokenizer Unicode characters in attribute values" {
    var gpa = std.heap.GeneralPurposeAllocator(.{}){};
    defer _ = gpa.deinit();
    const allocator = gpa.allocator();

    // æµ‹è¯•Unicodeå­—ç¬¦åœ¨å±æ€§å€¼ä¸­
    const html_input = "<div title=\"ä½ å¥½ä¸–ç•Œ\">Content</div>";
    var tok = tokenizer.Tokenizer.init(html_input, allocator);

    const token1_opt = try tok.next();
    try std.testing.expect(token1_opt != null);
    var token1 = token1_opt.?;
    defer token1.deinit();

    try std.testing.expect(token1.token_type == .start_tag);
    const title_attr = token1.data.start_tag.attributes.get("title");
    try std.testing.expect(title_attr != null);
    if (title_attr) |attr| {
        // éªŒè¯Unicodeå­—ç¬¦è¢«æ­£ç¡®è§£æ
        try std.testing.expect(attr.len > 0);
    }
}

test "tokenizer Unicode characters in text" {
    var gpa = std.heap.GeneralPurposeAllocator(.{}){};
    defer _ = gpa.deinit();
    const allocator = gpa.allocator();

    // æµ‹è¯•Unicodeå­—ç¬¦åœ¨æ–‡æœ¬ä¸­
    const html_input = "ä½ å¥½ä¸–ç•Œ";
    var tok = tokenizer.Tokenizer.init(html_input, allocator);

    const token1_opt = try tok.next();
    try std.testing.expect(token1_opt != null);
    var token1 = token1_opt.?;
    defer token1.deinit();

    try std.testing.expect(token1.token_type == .text);
    // éªŒè¯Unicodeå­—ç¬¦è¢«æ­£ç¡®è§£æ
    try std.testing.expect(token1.data.text.len > 0);
}

test "tokenizer emoji in text" {
    var gpa = std.heap.GeneralPurposeAllocator(.{}){};
    defer _ = gpa.deinit();
    const allocator = gpa.allocator();

    // æµ‹è¯•emojiå­—ç¬¦åœ¨æ–‡æœ¬ä¸­
    const html_input = "Hello ğŸ˜€ World ğŸŒ";
    var tok = tokenizer.Tokenizer.init(html_input, allocator);

    const token1_opt = try tok.next();
    try std.testing.expect(token1_opt != null);
    var token1 = token1_opt.?;
    defer token1.deinit();

    try std.testing.expect(token1.token_type == .text);
    // éªŒè¯emojiå­—ç¬¦è¢«æ­£ç¡®è§£æ
    try std.testing.expect(token1.data.text.len > 0);
}

test "tokenizer emoji in attribute values" {
    var gpa = std.heap.GeneralPurposeAllocator(.{}){};
    defer _ = gpa.deinit();
    const allocator = gpa.allocator();

    // æµ‹è¯•emojiå­—ç¬¦åœ¨å±æ€§å€¼ä¸­
    const html_input = "<div title=\"Hello ğŸ˜€ World ğŸŒ\">Content</div>";
    var tok = tokenizer.Tokenizer.init(html_input, allocator);

    const token1_opt = try tok.next();
    try std.testing.expect(token1_opt != null);
    var token1 = token1_opt.?;
    defer token1.deinit();

    try std.testing.expect(token1.token_type == .start_tag);
    const title_attr = token1.data.start_tag.attributes.get("title");
    try std.testing.expect(title_attr != null);
    if (title_attr) |attr| {
        // éªŒè¯emojiå­—ç¬¦è¢«æ­£ç¡®è§£æ
        try std.testing.expect(attr.len > 0);
    }
}

test "tokenizer InvalidTag error" {
    var gpa = std.heap.GeneralPurposeAllocator(.{}){};
    defer _ = gpa.deinit();
    const allocator = gpa.allocator();

    // æµ‹è¯•ç©ºæ ‡ç­¾åï¼ˆInvalidTagé”™è¯¯ï¼‰
    const html_input = "<>";
    var tok = tokenizer.Tokenizer.init(html_input, allocator);

    // åº”è¯¥è¿”å›InvalidTagé”™è¯¯ï¼ˆç©ºæ ‡ç­¾åï¼‰
    const token_opt = tok.next();
    try std.testing.expectError(error.InvalidTag, token_opt);
}

test "tokenizer InvalidTag error with whitespace" {
    var gpa = std.heap.GeneralPurposeAllocator(.{}){};
    defer _ = gpa.deinit();
    const allocator = gpa.allocator();

    // æµ‹è¯•åªæœ‰ç©ºç™½å­—ç¬¦çš„æ ‡ç­¾åï¼ˆInvalidTagé”™è¯¯ï¼‰
    const html_input = "< >";
    var tok = tokenizer.Tokenizer.init(html_input, allocator);

    // åº”è¯¥è¿”å›InvalidTagé”™è¯¯ï¼ˆç©ºæ ‡ç­¾åï¼‰
    const token_opt = tok.next();
    try std.testing.expectError(error.InvalidTag, token_opt);
}

test "tokenizer InvalidTag error with end tag" {
    var gpa = std.heap.GeneralPurposeAllocator(.{}){};
    defer _ = gpa.deinit();
    const allocator = gpa.allocator();

    // æµ‹è¯•ç©ºç»“æŸæ ‡ç­¾åï¼ˆInvalidTagé”™è¯¯ï¼‰
    const html_input = "</>";
    var tok = tokenizer.Tokenizer.init(html_input, allocator);

    // åº”è¯¥è¿”å›InvalidTagé”™è¯¯ï¼ˆç©ºæ ‡ç­¾åï¼‰
    const token_opt = tok.next();
    try std.testing.expectError(error.InvalidTag, token_opt);
}
